{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bfafa9-9b6e-4456-b57a-a8e517f1ff0d",
   "metadata": {},
   "source": [
    "# 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fa9c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import dnnlib, legacy\n",
    "import clip\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b301c9d-3cee-4ebd-adc4-c05c9f8a4827",
   "metadata": {},
   "source": [
    "# text2image generator 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93b1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, device, path):\n",
    "        self.name = 'generator'\n",
    "        self.model = self.load_model(device, path)\n",
    "        self.device = device\n",
    "        self.force_32 = False\n",
    "        \n",
    "    def load_model(self, device, path):\n",
    "        with dnnlib.util.open_url(path) as f:\n",
    "            network= legacy.load_network_pkl(f)\n",
    "            self.G_ema = network['G_ema'].to(device)\n",
    "            self.D = network['D'].to(device)\n",
    "#                 self.G = network['G'].to(device)\n",
    "            return self.G_ema\n",
    "        \n",
    "    def generate(self, z, c, fts, noise_mode='const', return_styles=True):\n",
    "        return self.model(z, c, fts=fts, noise_mode=noise_mode, return_styles=return_styles, force_fp32=self.force_32)\n",
    "    \n",
    "    def generate_from_style(self, style, noise_mode='const'):\n",
    "        ws = torch.randn(1, self.model.num_ws, 512)\n",
    "        return self.model.synthesis(ws, fts=None, styles=style, noise_mode=noise_mode, force_fp32=self.force_32)\n",
    "    \n",
    "    def tensor_to_img(self, tensor):\n",
    "        img = torch.clamp((tensor + 1.) * 127.5, 0., 255.)\n",
    "        img_list = img.permute(0, 2, 3, 1)\n",
    "        img_list = [img for img in img_list]\n",
    "        return Image.fromarray(torch.cat(img_list, dim=-2).detach().cpu().numpy().astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abffc1f7-24ee-4d71-aa86-50cdab548e8e",
   "metadata": {},
   "source": [
    "# clip score & rank 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3be2a158-c0f2-4db0-a3fa-ec52872b2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def clip_score_rank(prompt: str,\n",
    "               images: np.ndarray,\n",
    "               model_clip: torch.nn.Module,\n",
    "               preprocess_clip,\n",
    "               device: str) -> np.ndarray:\n",
    "    images = [preprocess_clip(Image.fromarray((image*255).astype(np.uint8))) for image in images]\n",
    "    images = torch.stack(images, dim=0).to(device=device)\n",
    "    texts = clip.tokenize(prompt).to(device=device)\n",
    "    texts = torch.repeat_interleave(texts, images.shape[0], dim=0)\n",
    "\n",
    "    image_features = model_clip.encode_image(images)\n",
    "    text_features = model_clip.encode_text(texts)\n",
    "\n",
    "    scores = F.cosine_similarity(image_features, text_features).squeeze()\n",
    "    rank = torch.argsort(scores, descending=True).cpu().numpy()\n",
    "    return scores, rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526af76-192c-4ab0-ad0c-22bed10ceb31",
   "metadata": {},
   "source": [
    "# 텍스트 입력 및 생성할 이미지 개수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa3378e7-8eee-402f-a1bc-df77594db369",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'an armchair in the shape of an avocado'\n",
    "num_images_to_generate = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975badc-ac11-4f6a-a1df-c1a687f9b39c",
   "metadata": {},
   "source": [
    "# lafite 실행 및 clip score & rank 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29873681",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    device = 'cuda:0' # please use GPU, do not use CPU\n",
    "    path = 'pre-trained-google-cc-best-fid.pkl'\n",
    "    # path = './some_pre-trained_models.pkl'  # pre-trained model\n",
    "    generator = Generator(device=device, path=path)\n",
    "    clip_model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "    clip_model = clip_model.eval()\n",
    "    \n",
    "    if num_images_to_generate > 1:\n",
    "        tokenized_text = clip.tokenize([txt]*num_images_to_generate).to(device)\n",
    "        txt_fts = clip_model.encode_text(tokenized_text)\n",
    "        txt_fts = txt_fts/txt_fts.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        z = torch.randn((num_images_to_generate, 512)).to(device)\n",
    "        c = torch.randn((num_images_to_generate, 1)).to(device) # label is actually not used\n",
    "        img, _ = generator.generate(z=z, c=c, fts=txt_fts)\n",
    "        to_show_img = generator.tensor_to_img(img)\n",
    "        to_show_img.save('./generated.jpg')\n",
    "    else:\n",
    "        tokenized_text = clip.tokenize([txt]).to(device)\n",
    "        txt_fts = clip_model.encode_text(tokenized_text)\n",
    "        txt_fts = txt_fts/txt_fts.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        z = torch.randn((1, 512)).to(device)\n",
    "        c = torch.randn((1, 1)).to(device) # label is actually not used\n",
    "        img, _ = generator.generate(z=z, c=c, fts=txt_fts)\n",
    "        to_show_img = generator.tensor_to_img(img)\n",
    "        to_show_img.save('./generated.jpg')\n",
    "    \n",
    "    clip_model.to(device=device)\n",
    "    scores, rank = clip_score_rank(prompt=txt,\n",
    "                  images=[np.array(to_show_img)],\n",
    "                  model_clip=clip_model,\n",
    "                  preprocess_clip=preprocess_clip,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0dec32-cae1-47d3-8388-8f9a09fc4357",
   "metadata": {},
   "source": [
    "* 이미지 tensor의 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0f43707-b20b-47f7-9883-6118b1a52094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f5bbb3-d18b-4880-9e45-f772bab738c8",
   "metadata": {},
   "source": [
    "* 이미지를 ndarray로 변환한 것(함수 clip_score_rank의 images에 해당하는 input 형태)의 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50c0cd36-8edb-4398-8bf2-9bc98607b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(to_show_img).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de9897e-5857-4b82-a305-c8264bcc4781",
   "metadata": {},
   "source": [
    "# lafite 실행 결과 이미지에 대한 clip score\n",
    "\n",
    "> 문장 : 'an armchair in the shape of an avocado'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aec165ab-88c1-4fbb-b3de-02603e0e7494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2744, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82701453-544b-4242-af4a-597e3cda5855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
