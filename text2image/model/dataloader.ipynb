{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1f6356-9225-4944-90f1-28b3d967aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "import random\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5687740a-f165-49c1-b27a-773e735c0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    동일한 조건으로 학습을 할 때, 동일한 결과를 얻기 위해 seed를 고정시킵니다.\n",
    "    \n",
    "    Args:\n",
    "        seed: seed 정수값\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed) # pytorch의 random seed 고정\n",
    "    torch.cuda.manual_seed(seed) # GPU 에서 사용하는 난수 생성 시드 고정\n",
    "    # torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True # CuDNN 부분고정\n",
    "    torch.backends.cudnn.benchmark = False # CuDNN 부분고정\n",
    "    np.random.seed(seed) # Numpy 부분\n",
    "    random.seed(seed) # transforms에서 random 라이브러리를 사용하기 때문에 random 라이브러리를 불러서 고정\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55110126-70f2-4453-9e99-d94f21eef05e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41847e38-3429-46bc-a5ea-f4b6590e5927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 개수 : 19495\n",
      "file 개수 : 21958\n",
      "file 개수 : 14799\n"
     ]
    }
   ],
   "source": [
    "def search(dirname, result):  # 하위목록의 모든 파일을 찾는 함수\n",
    "    try:\n",
    "        filenames = os.listdir(dirname)\n",
    "        print(f'file 개수 : {len(filenames)}')\n",
    "        for filename in filenames:\n",
    "            full_filename = os.path.join(dirname, filename)\n",
    "            if os.path.isdir(full_filename):\n",
    "                if full_filename.startswith('.'):\n",
    "                    continue\n",
    "                search(full_filename, result)\n",
    "            else:\n",
    "                ext = os.path.splitext(full_filename)[-1]  # 확장자 체크\n",
    "                if ext:\n",
    "                    result.append(full_filename)\n",
    "                else:\n",
    "                    print(full_filename)\n",
    "    except PermissionError:\n",
    "        print('error')\n",
    "\n",
    "illust_all_path = []\n",
    "search(\"./img_data/illustrations\", illust_all_path)\n",
    "scenery_all_path = []\n",
    "search(\"./img_data/scenery\", scenery_all_path)\n",
    "vector_all_path = []\n",
    "search(\"./img_data/vectors\", vector_all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2275d8e3-4e77-4217-95cd-3e11752b9d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19494, 21958, 14799)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(illust_all_path), len(scenery_all_path), len(vector_all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed21131-47ff-4676-88e3-1b0aa6053d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./img_data/vectors/phone-6032011__340.png',\n",
       " './img_data/vectors/fireplace-160963__340.png',\n",
       " './img_data/vectors/lighthouse-5575259__340.png',\n",
       " './img_data/vectors/dices-160654__340.png',\n",
       " './img_data/vectors/beaker-159176__340.png']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_all_path[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d5e4e21-b84d-4132-a3cc-c1f0ea26394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(dirname, result, prefix):  # 라벨링하는 함수\n",
    "    try:\n",
    "        filenames = os.listdir(dirname)\n",
    "        for filename in filenames:\n",
    "            if filename.startswith('.'):\n",
    "                continue\n",
    "            tmp_str = filename.split(\".\")[0].split(\"__\")[0]\n",
    "            keyword = prefix + ' '.join(tmp_str.split(\"-\")[:-1])\n",
    "            result.append(keyword)\n",
    "    except PermissionError:\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "972fc278-eb48-40ed-a657-a12d7b11a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname, illust_label = \"./img_data/illustrations\", []\n",
    "labeling(dirname, illust_label, \"an illustration image of \")\n",
    "dirname, vector_label = \"./img_data/vectors\", []\n",
    "labeling(dirname, vector_label, \"a vector image of \")\n",
    "dirname, scenery_label = \"./img_data/scenery\", []\n",
    "labeling(dirname, scenery_label, \"a scenery of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83aa34e9-f543-48e5-926c-84f7f22a5d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19494, 21958, 14799)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(illust_label), len(scenery_label),len(vector_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36cb374f-315e-4b65-96ec-d544512206e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an illustration image of bicycle',\n",
       " 'an illustration image of cave',\n",
       " 'an illustration image of watercolour',\n",
       " 'an illustration image of digital paper',\n",
       " 'an illustration image of cartoon']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "illust_label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de012903-cfb0-4d81-b418-8776b31c83e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./img_data/illustrations/bicycle-4307184__340.png</td>\n",
       "      <td>an illustration image of bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./img_data/illustrations/cave-5523266__340.png</td>\n",
       "      <td>an illustration image of cave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./img_data/illustrations/watercolour-4744419__...</td>\n",
       "      <td>an illustration image of watercolour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./img_data/illustrations/digital-paper-5262381...</td>\n",
       "      <td>an illustration image of digital paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./img_data/illustrations/cartoon-5852530__340.png</td>\n",
       "      <td>an illustration image of cartoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19489</th>\n",
       "      <td>./img_data/illustrations/gold-foil-tree-of-lif...</td>\n",
       "      <td>an illustration image of gold foil tree of life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19490</th>\n",
       "      <td>./img_data/illustrations/santa-claus-5785751__...</td>\n",
       "      <td>an illustration image of santa claus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19491</th>\n",
       "      <td>./img_data/illustrations/double-decker-3283422...</td>\n",
       "      <td>an illustration image of double decker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19492</th>\n",
       "      <td>./img_data/illustrations/swing-6143981__340.png</td>\n",
       "      <td>an illustration image of swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19493</th>\n",
       "      <td>./img_data/illustrations/vintage-christmas-car...</td>\n",
       "      <td>an illustration image of vintage christmas card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19494 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  \\\n",
       "0      ./img_data/illustrations/bicycle-4307184__340.png   \n",
       "1         ./img_data/illustrations/cave-5523266__340.png   \n",
       "2      ./img_data/illustrations/watercolour-4744419__...   \n",
       "3      ./img_data/illustrations/digital-paper-5262381...   \n",
       "4      ./img_data/illustrations/cartoon-5852530__340.png   \n",
       "...                                                  ...   \n",
       "19489  ./img_data/illustrations/gold-foil-tree-of-lif...   \n",
       "19490  ./img_data/illustrations/santa-claus-5785751__...   \n",
       "19491  ./img_data/illustrations/double-decker-3283422...   \n",
       "19492    ./img_data/illustrations/swing-6143981__340.png   \n",
       "19493  ./img_data/illustrations/vintage-christmas-car...   \n",
       "\n",
       "                                                 label  \n",
       "0                     an illustration image of bicycle  \n",
       "1                        an illustration image of cave  \n",
       "2                 an illustration image of watercolour  \n",
       "3               an illustration image of digital paper  \n",
       "4                     an illustration image of cartoon  \n",
       "...                                                ...  \n",
       "19489  an illustration image of gold foil tree of life  \n",
       "19490             an illustration image of santa claus  \n",
       "19491           an illustration image of double decker  \n",
       "19492                   an illustration image of swing  \n",
       "19493  an illustration image of vintage christmas card  \n",
       "\n",
       "[19494 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "illust_df = pd.DataFrame(illust_all_path, columns = ['path'])\n",
    "\n",
    "illust_df['label'] = illust_label\n",
    "display(illust_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65d31d-f5be-4fbe-a581-e0b70222804b",
   "metadata": {},
   "source": [
    "지우기 전 19498 rows × 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35565dc4-0147-49d4-8747-ff7d5fbf1999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./img_data/scenery/lindau-7124493__340.png</td>\n",
       "      <td>a scenery of lindau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./img_data/scenery/summer-845468__340.png</td>\n",
       "      <td>a scenery of summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./img_data/scenery/forest-4246141__340.png</td>\n",
       "      <td>a scenery of forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./img_data/scenery/water-549311__480.png</td>\n",
       "      <td>a scenery of water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./img_data/scenery/canyon-2420827__340.png</td>\n",
       "      <td>a scenery of canyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21953</th>\n",
       "      <td>./img_data/scenery/rock-outcrop-1599212__340.png</td>\n",
       "      <td>a scenery of rock outcrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21954</th>\n",
       "      <td>./img_data/scenery/cayambe-2635372__340.png</td>\n",
       "      <td>a scenery of cayambe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21955</th>\n",
       "      <td>./img_data/scenery/sea-3168282__340.png</td>\n",
       "      <td>a scenery of sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21956</th>\n",
       "      <td>./img_data/scenery/grass-3239456__340.png</td>\n",
       "      <td>a scenery of grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21957</th>\n",
       "      <td>./img_data/scenery/path-6619387__340.png</td>\n",
       "      <td>a scenery of path</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21958 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  \\\n",
       "0            ./img_data/scenery/lindau-7124493__340.png   \n",
       "1             ./img_data/scenery/summer-845468__340.png   \n",
       "2            ./img_data/scenery/forest-4246141__340.png   \n",
       "3              ./img_data/scenery/water-549311__480.png   \n",
       "4            ./img_data/scenery/canyon-2420827__340.png   \n",
       "...                                                 ...   \n",
       "21953  ./img_data/scenery/rock-outcrop-1599212__340.png   \n",
       "21954       ./img_data/scenery/cayambe-2635372__340.png   \n",
       "21955           ./img_data/scenery/sea-3168282__340.png   \n",
       "21956         ./img_data/scenery/grass-3239456__340.png   \n",
       "21957          ./img_data/scenery/path-6619387__340.png   \n",
       "\n",
       "                           label  \n",
       "0            a scenery of lindau  \n",
       "1            a scenery of summer  \n",
       "2            a scenery of forest  \n",
       "3             a scenery of water  \n",
       "4            a scenery of canyon  \n",
       "...                          ...  \n",
       "21953  a scenery of rock outcrop  \n",
       "21954       a scenery of cayambe  \n",
       "21955           a scenery of sea  \n",
       "21956         a scenery of grass  \n",
       "21957          a scenery of path  \n",
       "\n",
       "[21958 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenery_df = pd.DataFrame(scenery_all_path, columns = ['path'])\n",
    "\n",
    "scenery_df['label'] = scenery_label\n",
    "scenery_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8743729d-3dff-458e-afe5-5366f905a8d1",
   "metadata": {},
   "source": [
    "지우기 전 22125 rows × 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a31023e-d088-483a-bbe0-4f279e63571f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./img_data/vectors/phone-6032011__340.png</td>\n",
       "      <td>a vector image of phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./img_data/vectors/fireplace-160963__340.png</td>\n",
       "      <td>a vector image of fireplace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./img_data/vectors/lighthouse-5575259__340.png</td>\n",
       "      <td>a vector image of lighthouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./img_data/vectors/dices-160654__340.png</td>\n",
       "      <td>a vector image of dices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./img_data/vectors/beaker-159176__340.png</td>\n",
       "      <td>a vector image of beaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14794</th>\n",
       "      <td>./img_data/vectors/escape-4143232__340.png</td>\n",
       "      <td>a vector image of escape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14795</th>\n",
       "      <td>./img_data/vectors/forest-5616880__340.png</td>\n",
       "      <td>a vector image of forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14796</th>\n",
       "      <td>./img_data/vectors/dishes-576376__340.png</td>\n",
       "      <td>a vector image of dishes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14797</th>\n",
       "      <td>./img_data/vectors/bear-3544195__480.png</td>\n",
       "      <td>a vector image of bear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14798</th>\n",
       "      <td>./img_data/vectors/building-305378__340.png</td>\n",
       "      <td>a vector image of building</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14799 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path  \\\n",
       "0           ./img_data/vectors/phone-6032011__340.png   \n",
       "1        ./img_data/vectors/fireplace-160963__340.png   \n",
       "2      ./img_data/vectors/lighthouse-5575259__340.png   \n",
       "3            ./img_data/vectors/dices-160654__340.png   \n",
       "4           ./img_data/vectors/beaker-159176__340.png   \n",
       "...                                               ...   \n",
       "14794      ./img_data/vectors/escape-4143232__340.png   \n",
       "14795      ./img_data/vectors/forest-5616880__340.png   \n",
       "14796       ./img_data/vectors/dishes-576376__340.png   \n",
       "14797        ./img_data/vectors/bear-3544195__480.png   \n",
       "14798     ./img_data/vectors/building-305378__340.png   \n",
       "\n",
       "                              label  \n",
       "0           a vector image of phone  \n",
       "1       a vector image of fireplace  \n",
       "2      a vector image of lighthouse  \n",
       "3           a vector image of dices  \n",
       "4          a vector image of beaker  \n",
       "...                             ...  \n",
       "14794      a vector image of escape  \n",
       "14795      a vector image of forest  \n",
       "14796      a vector image of dishes  \n",
       "14797        a vector image of bear  \n",
       "14798    a vector image of building  \n",
       "\n",
       "[14799 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df = pd.DataFrame(vector_all_path, columns = ['path'])\n",
    "\n",
    "vector_df['label'] = vector_label\n",
    "vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37332b9-01a9-4618-a9bb-9ee21adc1308",
   "metadata": {},
   "source": [
    "지우기 전 14876 rows × 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72bc01d2-364f-42a1-b2c7-179630f5c434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./img_data/illustrations/bicycle-4307184__340.png</td>\n",
       "      <td>an illustration image of bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./img_data/illustrations/cave-5523266__340.png</td>\n",
       "      <td>an illustration image of cave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./img_data/illustrations/watercolour-4744419__...</td>\n",
       "      <td>an illustration image of watercolour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./img_data/illustrations/digital-paper-5262381...</td>\n",
       "      <td>an illustration image of digital paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./img_data/illustrations/cartoon-5852530__340.png</td>\n",
       "      <td>an illustration image of cartoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56246</th>\n",
       "      <td>./img_data/scenery/rock-outcrop-1599212__340.png</td>\n",
       "      <td>a scenery of rock outcrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56247</th>\n",
       "      <td>./img_data/scenery/cayambe-2635372__340.png</td>\n",
       "      <td>a scenery of cayambe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56248</th>\n",
       "      <td>./img_data/scenery/sea-3168282__340.png</td>\n",
       "      <td>a scenery of sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56249</th>\n",
       "      <td>./img_data/scenery/grass-3239456__340.png</td>\n",
       "      <td>a scenery of grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56250</th>\n",
       "      <td>./img_data/scenery/path-6619387__340.png</td>\n",
       "      <td>a scenery of path</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56251 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  \\\n",
       "0      ./img_data/illustrations/bicycle-4307184__340.png   \n",
       "1         ./img_data/illustrations/cave-5523266__340.png   \n",
       "2      ./img_data/illustrations/watercolour-4744419__...   \n",
       "3      ./img_data/illustrations/digital-paper-5262381...   \n",
       "4      ./img_data/illustrations/cartoon-5852530__340.png   \n",
       "...                                                  ...   \n",
       "56246   ./img_data/scenery/rock-outcrop-1599212__340.png   \n",
       "56247        ./img_data/scenery/cayambe-2635372__340.png   \n",
       "56248            ./img_data/scenery/sea-3168282__340.png   \n",
       "56249          ./img_data/scenery/grass-3239456__340.png   \n",
       "56250           ./img_data/scenery/path-6619387__340.png   \n",
       "\n",
       "                                        label  \n",
       "0            an illustration image of bicycle  \n",
       "1               an illustration image of cave  \n",
       "2        an illustration image of watercolour  \n",
       "3      an illustration image of digital paper  \n",
       "4            an illustration image of cartoon  \n",
       "...                                       ...  \n",
       "56246               a scenery of rock outcrop  \n",
       "56247                    a scenery of cayambe  \n",
       "56248                        a scenery of sea  \n",
       "56249                      a scenery of grass  \n",
       "56250                       a scenery of path  \n",
       "\n",
       "[56251 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([illust_df, vector_df, scenery_df], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f880c0-cb11-4aac-9f72-d5fe11551476",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 깨진 이미지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a962b3c-8eb5-440f-8312-e0b5ae342502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index in range(len(df)):\n",
    "#     try:\n",
    "#         image = Image.open(df['path'].iloc[index]).convert('RGB')\n",
    "#         label = df['label'].iloc[index]\n",
    "#     except e:\n",
    "#         print(e)\n",
    "#         print(f'Image load error : {df[\"path\"].iloc[index]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae2767-3d1a-40ec-b86b-8fa646c7597e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 깨진 이미지 지우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5989a6d-07d6-4577-9f41-30a27d310415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index in range(len(df)):\n",
    "#     try:\n",
    "#         image = Image.open(df['path'].iloc[index]).convert('RGB')\n",
    "#         label = df['label'].iloc[index]\n",
    "#     except:\n",
    "#         print(f'Image load error : {df[\"path\"].iloc[index]}')\n",
    "#         os.remove(df[\"path\"].iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc105b-4d62-4e32-9f80-18342833325c",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec4b529f-dbc6-48fc-852c-9ca888dae088",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'val':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d87d3aeb-0916-4737-bb31-cfb0f47ab485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_paths_label, transform):\n",
    "        self.X = img_paths_label['path']\n",
    "        self.y = img_paths_label['label']\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # image = Image.open(self.X.iloc[index])\n",
    "        try:\n",
    "            image = Image.open(self.X.iloc[index]).convert('RGB')\n",
    "            label = self.y.iloc[index]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except:\n",
    "            print(f'Image load error : {self.X.iloc[index]}')\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0d7d4-f579-4f3d-bf5d-bbba86694b5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c254e52f-6e94-411a-b723-7026f3c1638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, Callback\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.distributed import rank_zero_only\n",
    "\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "from dalle.models import ImageGPT, Dalle, Rep_Dalle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f0f45c3-6307-4a78-bde3-1823131a0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b7f412f-3ced-4855-a99f-b50d07e6597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "path_upstream = 'minDALL-E/1.3B'\n",
    "config_file = './configs/transfer-imagenet-uncond-gen.yaml'\n",
    "# config_file = './configs/transfer-imagenet-clscond-gen.yaml'\n",
    "config_downstream = config_file\n",
    "# with open(config_file) as f:\n",
    "#     config_downstream = yaml.load(f,Loader=yaml.FullLoader)\n",
    "\n",
    "result_path = './base_result'\n",
    "data_dir = '../img_data'\n",
    "\n",
    "n_gpus = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d302e75-1203-4f90-b084-aedfb87567e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLogger(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_img(self, pl_module, batch, current_epoch, split=\"train\"):\n",
    "        with torch.no_grad():\n",
    "            images, labels = batch\n",
    "            recons = pl_module.stage1(images)\n",
    "            images = images.cpu()\n",
    "            recons = recons.cpu()\n",
    "\n",
    "            grid_org = (torchvision.utils.make_grid(images, nrow=8) + 1.0) / 2.0\n",
    "            grid_rec = (torchvision.utils.make_grid(recons, nrow=8) + 1.0) / 2.0\n",
    "            grid_rec = torch.clip(grid_rec, min=0, max=1)\n",
    "\n",
    "            pl_module.logger.experiment.add_image(f\"images_org/{split}\", grid_org, global_step=current_epoch)\n",
    "            pl_module.logger.experiment.add_image(f\"images_rec/{split}\", grid_rec, global_step=current_epoch)\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        if batch_idx == 0 and trainer.current_epoch < 5:\n",
    "            self.log_img(pl_module, batch, current_epoch=trainer.current_epoch, split=\"train\")\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        if batch_idx == 0 and trainer.current_epoch < 5:\n",
    "            self.log_img(pl_module, batch, current_epoch=trainer.current_epoch, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92fee0b8-411a-4e11-8632-b4f34e224e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 data_dir: Optional[str] = None,\n",
    "                 image_resolution: int = 256,\n",
    "                 train_batch_size: int = 2,\n",
    "                 valid_batch_size: int = 32,\n",
    "                 num_workers: int = 8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.image_resolution = image_resolution\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.valid_batch_size = valid_batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.train_transform = transforms.Compose(\n",
    "            [transforms.Resize(image_resolution),\n",
    "             transforms.RandomCrop(image_resolution),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]\n",
    "        )\n",
    "        self.valid_transform = transforms.Compose(\n",
    "            [transforms.Resize(image_resolution),\n",
    "             transforms.CenterCrop(image_resolution),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.trainset = torchvision.datasets.ImageNet(root=self.data_dir, split='train', transform=self.train_transform)\n",
    "        self.validset = torchvision.datasets.ImageNet(root=self.data_dir, split='val', transform=self.valid_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainset,\n",
    "                          batch_size=self.train_batch_size,\n",
    "                          num_workers=0,\n",
    "                          # num_workers=self.num_workers,\n",
    "                          pin_memory=True)\n",
    "\n",
    "    def valid_dataloader(self):\n",
    "        return DataLoader(self.validset,\n",
    "                          batch_size=self.valid_batch_size,\n",
    "                          num_workers=0,\n",
    "                          # num_workers=self.num_workers,\n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26671d7f-ba86-4f94-a04c-b8282f600b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, valid = train_test_split(df, test_size=0.2,\n",
    "                               shuffle=True,\n",
    "                               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "725c08ab-8b9f-43c3-b478-b1a9347a5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 data_dir: Optional[str] = None,\n",
    "                 image_resolution: int = 256,\n",
    "                 train_batch_size: int = 2,\n",
    "                 valid_batch_size: int = 32,\n",
    "                 num_workers: int = 8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.image_resolution = image_resolution\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.valid_batch_size = valid_batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.train_transform = transforms.Compose(\n",
    "            [transforms.Resize(image_resolution),\n",
    "             transforms.RandomCrop(image_resolution),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]\n",
    "        )\n",
    "        self.valid_transform = transforms.Compose(\n",
    "            [transforms.Resize(image_resolution),\n",
    "             transforms.CenterCrop(image_resolution),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.trainset = CustomDataset(train, data_transforms['train'])\n",
    "        self.validset = CustomDataset(valid, data_transforms['val'])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainset,\n",
    "                          batch_size=self.train_batch_size,\n",
    "                          num_workers=0,\n",
    "                          # num_workers=self.num_workers,\n",
    "                          pin_memory=True)\n",
    "\n",
    "    def valid_dataloader(self):\n",
    "        return DataLoader(self.validset,\n",
    "                          batch_size=self.valid_batch_size,\n",
    "                          num_workers=0,\n",
    "                          # num_workers=self.num_workers,\n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eda92a42-ad8d-4a34-a181-6c561a2cd018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_callbacks(config, args_result_path):\n",
    "    # Setup callbacks\n",
    "    now = datetime.now().strftime('%d%m%Y_%H%M%S')\n",
    "    result_path = os.path.join(args_result_path,\n",
    "                               os.path.basename(config_downstream).split('.')[0],\n",
    "                               now)\n",
    "    ckpt_path = os.path.join(result_path, 'ckpt')\n",
    "    log_path = os.path.join(result_path, 'log')\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=ckpt_path,\n",
    "        filename=\"custom-clscond-gen-{epoch:02d}\" if config.stage2.use_cls_cond else\n",
    "                 \"custom-uncond-gen-{epoch:02d}\",\n",
    "        every_n_epochs=config.experiment.save_ckpt_freq,\n",
    "        save_weights_only=True,\n",
    "        save_last=True\n",
    "    )\n",
    "    logger = TensorBoardLogger(log_path, name=\"iGPT\")\n",
    "    logger_img = ImageLogger()\n",
    "    return checkpoint_callback, logger, logger_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c8bc6b3-c44e-45bf-8245-1571ad7716b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup_callbacks(config, args_result_path):\n",
    "#     # Setup callbacks\n",
    "#     now = datetime.now().strftime('%d%m%Y_%H%M%S')\n",
    "#     result_path = os.path.join(args_result_path,\n",
    "#                                'config',\n",
    "#                                now)\n",
    "#     ckpt_path = os.path.join(result_path, 'ckpt')\n",
    "#     log_path = os.path.join(result_path, 'log')\n",
    "\n",
    "#     checkpoint_callback = ModelCheckpoint(\n",
    "#         dirpath=ckpt_path,\n",
    "#         filename=\"customdata-clscond-gen-{epoch:02d}\" if config.stage2.use_cls_cond else\n",
    "#                  \"customdata-uncond-gen-{epoch:02d}\",\n",
    "#         every_n_epochs=config.experiment.save_ckpt_freq,\n",
    "#         save_weights_only=True,\n",
    "#         save_last=True\n",
    "#     )\n",
    "#     logger = TensorBoardLogger(log_path, name=\"iGPT\")\n",
    "#     logger_img = ImageLogger()\n",
    "#     return checkpoint_callback, logger, logger_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ccddb5e-d7be-4f1b-b57b-3f365cfdbd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     pl.seed_everything(seed)\n",
    "\n",
    "#     # Build iGPT\n",
    "#     model, config = ImageGPT.from_pretrained(path_upstream, config_downstream)\n",
    "#     model = Dalle.from_pretrained(path_upstream)\n",
    "#     config = config_downstream\n",
    "\n",
    "#     # Setup callbacks\n",
    "#     # ckpt_callback, logger, logger_img = setup_callbacks(config,result_path)\n",
    "\n",
    "#     # Build data modules\n",
    "#     dataset = CustomDataModule(data_dir=data_dir,\n",
    "#                                  # image_resolution=config.dataset.image_resolution,\n",
    "#                                  # train_batch_size=config.experiment.local_batch_size,\n",
    "#                                  # valid_batch_size=config.experiment.valid_batch_size,\n",
    "#                                  num_workers=16)\n",
    "#     dataset.setup()\n",
    "#     train_dataloader = dataset.train_dataloader()\n",
    "#     valid_dataloader = dataset.valid_dataloader()\n",
    "#     print(f\"len(train_dataset) = {len(dataset.trainset)}\")\n",
    "#     print(f\"len(valid_dataset) = {len(dataset.validset)}\")\n",
    "\n",
    "#     # Calculate how many batches are accumulated\n",
    "#     # assert config.experiment.total_batch_size % (config.experiment.local_batch_size * args.n_gpus) == 0\n",
    "#     grad_accm_steps = config['experiment']['total_batch_size'] // (config['experiment']['local_batch_size'] * n_gpus)\n",
    "#     config['optimizer']['max_steps'] = len(dataset.trainset) // config['experiment']['total_batch_size'] * config['experiment']['epochs']\n",
    "\n",
    "#     # Build trainer\n",
    "#     trainer = pl.Trainer(max_epochs=config['experiment']['epochs'],\n",
    "#                          accumulate_grad_batches=grad_accm_steps,\n",
    "#                          gradient_clip_val=config['optimizer']['grad_clip_norm'],\n",
    "#                          precision=32, # amp -> 16 아니면 32\n",
    "#                          # callbacks=[ckpt_callback, logger_img],\n",
    "#                          accelerator=\"gpu\",\n",
    "#                          devices=n_gpus,\n",
    "#                          # strategy=\"ddp\",\n",
    "#                          # logger=logger\n",
    "#                         )\n",
    "#     trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ed2eb78-9292-476b-acec-0a8b685f5340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/.cache/minDALL-E/1.3B/tokenizer successfully restored..\n",
      "/opt/ml/.cache/minDALL-E/1.3B/stage1_last.ckpt successfully restored..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:117: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:342: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:307: LightningDeprecationWarning: The `LightningModule.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `LightningModule.on_<train/validation/test>_epoch_start` instead.\n",
      "  rank_zero_deprecation(\n",
      "Missing logger folder: ./base_result/transfer-imagenet-uncond-gen/29052022_081704/log/iGPT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/.cache/minDALL-E/1.3B/stage2_last.ckpt successfully restored..\n",
      "len(train_dataset) = 45000\n",
      "len(valid_dataset) = 11251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type          | Params\n",
      "-----------------------------------------\n",
      "0 | stage1 | VQGAN         | 76.1 M\n",
      "1 | stage2 | Transformer1d | 1.3 B \n",
      "-----------------------------------------\n",
      "1.3 B     Trainable params\n",
      "76.1 M    Non-trainable params\n",
      "1.4 B     Total params\n",
      "2,734.311 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n",
      "torch.Size([32, 256, 16384])\n",
      "torch.Size([32, 63, 16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n",
      "torch.Size([32, 256, 16384])\n",
      "torch.Size([32, 63, 16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ee06a70f85452aace99117446f98ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pl.seed_everything(seed)\n",
    "\n",
    "    # Build iGPT\n",
    "    model, config = Rep_Dalle.from_pretrained(path_upstream, config_downstream)\n",
    "\n",
    "    # Setup callbacks\n",
    "    ckpt_callback, logger, logger_img = setup_callbacks(config,result_path)\n",
    "\n",
    "    # Build data modules\n",
    "    dataset = CustomDataModule(data_dir=data_dir,\n",
    "                                 image_resolution=config.dataset.image_resolution,\n",
    "                                 train_batch_size=config.experiment.local_batch_size,\n",
    "                                 valid_batch_size=config.experiment.valid_batch_size,\n",
    "                                 num_workers=16)\n",
    "    dataset.setup()\n",
    "    train_dataloader = dataset.train_dataloader()\n",
    "    valid_dataloader = dataset.valid_dataloader()\n",
    "    print(f\"len(train_dataset) = {len(dataset.trainset)}\")\n",
    "    print(f\"len(valid_dataset) = {len(dataset.validset)}\")\n",
    "\n",
    "    # Calculate how many batches are accumulated\n",
    "    assert config.experiment.total_batch_size % (config.experiment.local_batch_size * n_gpus) == 0\n",
    "    grad_accm_steps = config.experiment.total_batch_size // (config.experiment.local_batch_size * n_gpus)\n",
    "    config.optimizer.max_steps = len(dataset.trainset) // config.experiment.total_batch_size * config.experiment.epochs\n",
    "    \n",
    "    # Build trainer\n",
    "    trainer = pl.Trainer(max_epochs=config.experiment.epochs,\n",
    "                         accumulate_grad_batches=grad_accm_steps,\n",
    "                         gradient_clip_val=config.optimizer.grad_clip_norm,\n",
    "                         precision=16 if config.experiment.use_amp else 32,\n",
    "                         callbacks=[ckpt_callback, logger_img],\n",
    "                         accelerator=\"gpu\",\n",
    "                         devices=n_gpus,\n",
    "                         # strategy=\"ddp\",\n",
    "                         logger=logger)\n",
    "    trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b89bf7b-2851-4c32-a2c0-03b5a1f56567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "856c9d0e-5894-444d-92cb-ee1c37d11fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%rm -rf ~/.local/share/Trash/files/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c22fd-84ba-444e-aa27-fab019442397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
